\section{Data Integration}\label{sec:ekgmm-b-2-5} % B.2.5 Data Integration

Data integration is the process of combining data from different sources into a single, unified view for
business consumption and enhanced utility.
The process of integrating data from multiple sources begins with the ingestion process and may include activities
such as data profiling, cleansing/remediation, cross-referencing transformation, and field mapping.

\ekgmmContextSection

Data integration is challenging because data from multiple sources can have different file formats, data structures,
definitions, and contextual meanings.
Using the knowledge graph during data integration standardizes the meaning of data and makes the content understandable
to both humans and machines.
Embedding referenceable meaning into the data using machine-readable standards facilitates automatic validation and
assurance of \iindex{data quality}.
Registering data integration activities into the knowledge graph generates full data transparency across
linked processes.
Finally, ontology-based metadata representations make it possible to embed business rules and accommodate
different values, identities, and definitions that existed at various times in the entity lifecycle.

\kgmmcorequestionssection

\begin{core-questions}

  \item [\thesection.1] Are the data integration activities, their systems, repositories, and connections
                        known and tracked
  \item [\thesection.2] Are data integration activities linked to data inventory, business glossaries, and data models
  \item [\thesection.3] Are all data integration input and output datasets documented, tracked, and governed
  \item [\thesection.4] Are there reusable standards and defined business rules for performing data integration
  \item [\thesection.5] Are data integration patterns, tools, and technologies defined, governed, and used
  \item [\thesection.6] Has the firm established a central data integration function
                        (i.e. integration Center of Excellence) to manage \glsxtrshort{etl} across both
                        internal and external data pipelines
\end{core-questions}

\kgmmcorequestionssection

The goal is not always a single source of data - but rather the ability to choose the right authoritative source
for the appropriate context.

\kgmmscoringsection

\kgmmscoringlevelOne

\begin{scoring}

    \item All data sources are identified and documented for in-scope use cases
    \item Do we know the authoritative source for each data set (should not be able to do integration without
          using approved authoritative source)
    \item Does everyone agree that we are using the right sources (the right source for every context) --
          link to governance
    \item Do we have an approved list of what each source feeds (precise description at the entity level that
          we can get from an approved source\,---\,must know if this is the primary source of the data per the
          use case context).
          For any given entity do I have all the potential sources and for a specific context do I know which
          is authorized.
    \item There is a defined governance process for change management and testing (clear picture of all the
          dependencies for data integration).
          If there are changes to authoritative sources\,---\,do we know the downstream implications (tracked and tested)
    \item Are all \glspl{technology-stack} known and supported by current teams (are all key systems under the
          management and governance of the organization\,---\,should not have ghost systems that are not controlled
          as part of the integration process)
    \item Entitlement policies and classification rules (i.e. security, PII, business sensitive) are
          defined and verified
    \item \iindex{Data quality} requirements are defined, documented, and verified

\end{scoring}

\kgmmscoringlevelTwo

\begin{scoring}

    \item All information (above) are identified, precisely defined, and on-boarded into the knowledge graph
    \item Able to do datapoint lineage (detailed and complete view of the data integration landscape)
    \item Start making the \gls{ekg} the central point for data integration (the \gls{ekg} becomes the Rosetta stone of
          integration)\,---\,onboard systems, convert to RDF, integrate into \gls{ekg} (defined as the
          data integration strategy\,---\,not necessarily complete)
    \item All data sets that are on-boarded into the \gls{ekg} are coming from the authoritative sources.
          There are no man-in-the-middle systems.
          The goal is direct from the authoritative source to the target system for in-scope use cases.
          Must get the most granular data directly from the authoritative sources.
    \item All datasets are "\glspl{ekg:sdd}".
    \item [policy] all data is obtained from the \gls{ekg} as the authoritative source.
          Do not go directly to the originating source of the data.
    \item Entitlement policies and classification requirements are on-boarded into the \gls{ekg}
    \item \hyperref[sec:ekg-mm-data-quality-business-rules]{Data quality business rules}
          are on-boarded into the \gls{ekg}

\end{scoring}

\kgmmscoringlevelThree

\begin{scoring}

    \item Data is precisely defined (granular level) - expressed as formal ontologies - and on-boarded into the \gls{ekg}
    \item All data flows are modeled, defined, and registered in the \gls{ekg} (full lineage in the \gls{ekg} for all
          in-scope applications)
    \item Start to make the \gls{ekg} the authoritative source (set-up to facilitate decommissioning of systems).
          The \gls{ekg} is structured to become the “new” system for in-scope applications (as soon as all
          connections emanate from the \gls{ekg}).
    \item Entitlements are automatically managed enforced

\end{scoring}

\kgmmscoringlevelFour

\begin{scoring}

    \item [policy] All downstream client systems are using authoritative sources as the only source of information
          for in-scope datasets (\gls{ekg} is in the middle of all data flows)
    \item All “\iindex{cottage industry} systems” are replaced by the \gls{ekg}
          (and \gls{ekg} is able to perform all the requirements of any system it replaced --
          reporting, entitlement, quality control)

\end{scoring}
