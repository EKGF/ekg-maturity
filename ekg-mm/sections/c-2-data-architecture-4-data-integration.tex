%
% C.2.4 Data Mapping (formerly known as Data Integration & Interoperability)
%
\ekgmmCapability{c-2-4}{data-mapping}{Data Mapping}
\index{data!mapping}

\ekgmmCapabilityContributionToEnterprise{c-2-4}

\ekgmmCapabilityContributionToEKG{c-2-4}

\begin{maturity-dimensions}

  \item Are the data integration activities, their systems, repositories, and connections
        known and tracked
  \item Are data integration activities linked to data inventory, business glossaries, and data models
  \item Are all data integration input and output datasets documented, tracked, and governed
  \item Are there reusable standards and defined business rules for performing data integration
  \item Are data integration patterns, tools, and technologies defined, governed, and used
  \item Has the firm established a central data integration function
        (i.e. integration Center of Excellence) to manage \glsxtrshort{etl} across both
        internal and external data pipelines
\end{maturity-dimensions}

The goal is not always a single source of data - but rather the ability to choose the right authoritative source
for the appropriate context.

\ekgmmCapabilitySectionLevelsOneFive

\begin{level-assessment}{c-2-4}{1}

    \item All data sources are identified and documented for in-scope use cases
    \item Do we know the authoritative source for each data set (should not be able to do integration without
          using approved authoritative source)
    \item Does everyone agree that we are using the right sources (the right source for every context) --
          link to governance
    \item Do we have an approved list of what each source feeds (precise description at the entity level that
          we can get from an approved source\,---\,must know if this is the primary source of the data per the
          use case context).
          For any given entity do I have all the potential sources and for a specific context do I know which
          is authorized.
    \item There is a defined governance process for change management and testing (clear picture of all the
          dependencies for data integration).
          If there are changes to authoritative sources\,---\,do we know the downstream implications (tracked and tested)
    \item Are all \glspl{technology-stack} known and supported by current teams (are all key systems under the
          management and governance of the organization\,---\,should not have ghost systems that are not controlled
          as part of the integration process)
    \item Entitlement policies\index{entitlement!policies} and classification rules (i.e. security, PII,
          business sensitive) are defined and verified
    \item Data Quality\index{data!quality} requirements are defined, documented, and verified

\end{level-assessment}

\begin{level-assessment}{c-2-4}{2}

    \item All information (above) are identified, precisely defined, and on-boarded into the knowledge graph
    \item Able to do datapoint lineage (detailed and complete view of the data integration landscape)
    \item Start making the \gls{ekg} the central point for data integration (the \gls{ekg} becomes the Rosetta stone of
          integration)\,---\,onboard systems, convert to RDF, integrate into \gls{ekg} (defined as the
          data integration strategy\,---\,not necessarily complete)
    \item All data sets that are on-boarded into the \gls{ekg} are coming from the authoritative sources.
          There are no man-in-the-middle systems.
          The goal is direct from the authoritative source to the target system for in-scope use cases.
          Must get the most granular data directly from the authoritative sources.
    \item All datasets are "\glspl{sdd}".
    \item \textbf{Policy}\,---\,All data is obtained from the \gls{ekg} as the authoritative source.
          Do not go directly to the originating source of the data.
    \item Entitlement policies and classification requirements are on-boarded into the \gls{ekg}
    \item \hyperref[sec:ekg-mm-data-quality-business-rules]{Data quality business rules}
          are on-boarded into the \gls{ekg}

\end{level-assessment}

\begin{level-assessment}{c-2-4}{3}

    \item Data is precisely defined (granular level) - expressed as formal ontologies - and on-boarded into the \gls{ekg}
    \item All data flows are modeled, defined, and registered in the \gls{ekg} (full lineage in the \gls{ekg} for all
          in-scope applications)
    \item Start to make the \gls{ekg} the authoritative source (set-up to facilitate decommissioning of systems).
          The \gls{ekg} is structured to become the “new” system for in-scope applications (as soon as all
          connections emanate from the \gls{ekg}).
    \item Entitlements are automatically managed enforced

\end{level-assessment}

\begin{level-assessment}{c-2-4}{4}

    \item \textbf{policy}\,---\,All downstream client systems are using authoritative sources as the only source of information
          for in-scope datasets (\gls{ekg} is in the middle of all data flows)
    \item All “\iindex{cottage industry} systems” are replaced by the \gls{ekg}
          (and \gls{ekg} is able to perform all the requirements of any system it replaced --
          reporting, entitlement, quality control)

\end{level-assessment}
